# Run the Newton-Raphson method
NR.gam(y)
log(theta_0)
(n*kappa_0*log(theta_0))
(n*log(theta_0))
library(tidyverse)
library(MASS) # for mvrnorm
library(mvnfast) # for dmvn
library(Matrix)
library(caret)
library(stats)
library(Amelia)
library(mice)
library(bootstrap)
library(boot)
library(faux)
library(glmnet)
library(hierNet)
library(ncvreg)
library(RAMP)
library(randomForest)
library(ranger)       # a faster implementation of randomForest
library(Boruta)       # Selection of important features for RF
library(iRF)
library(MFSIS)
library(vip)
library(reshape)
library(reshape2)
library(Rfast)
library(mccr)
library(SimDesign) # package for rmvnorm
library(Metrics) # # Import library for Metrics for MAE MSE and Rsquared values
library(parallel)
library(doParallel)
?idcsi
library(markdown)
library(readr)
library(excel.link)
library(Amelia)
library(mice)
library(viridis)
library(hrbrthemes)
library(RColorBrewer)
library(MASS)
library(psych)
library(psycho)
library(Matrix)
library(data.table) # for fread(), a faster read.table.
library(tidyverse) # ggplot and associated packages.
library(caret) # confusionMatrix() and createDataPartition().
library(corrplot) # corrplot() to visualize correlation between variables.
library(rpart) # rpart() for the decision tree model.
library(rpart.plot) # to plot the rpart() model.
library(pROC) # roc() for getting the AUC of the ROC.
library(gbm) # for gbm() and the associated functions.
library(xgboost) # for xgboost().
library(precrec) # for evalmod() to find AUPRC.
library(kableExtra)
library(parallel)
library(doParallel)
library(GGally)
library(RAMP)
library(randomForest)
library(class)
library(graphics)
library(glmnet)
library(ncvreg)
library(stats)
library(iRF)
library(RCurl) #Library to read data from a URL
library(caret)
library(ROSE)
library(tictoc)
DF.1 <- read_table2("C:/Users/13363/Desktop/Proposal Result/Results/NewDissertCode/RealData/Wisconsin_Breast_Cancer/Imbalanced/training_data/Features.txt", col_names = FALSE)
DF.2 <- read_table2("C:/Users/13363/Desktop/Proposal Result/Results/NewDissertCode/RealData/Wisconsin_Breast_Cancer/Imbalanced/training_data/Info.txt", col_names = FALSE)
DF.3 <- DF.2 %>%
rename(BC = X1) %>%
dplyr::select(BC)
DF.Full <- bind_cols(DF.3, DF.1) %>%
dplyr::select(-X118)
glimpse(DF.Full)
DF.UnB <- DF.Full %>%
dplyr::select(BC, contains("X"))%>%
mutate(diagnosis = ifelse(BC == "-1", 0, ifelse(BC == "1", 1, NA)),
diagnosis = as.factor(diagnosis)) %>%
dplyr::select(-BC)
write.csv(DF.UnB, "C:/Users/13363/Documents/Semesters/SPRING2024/Splunk/Code/Wisconsin_Imbal.csv", row.names = TRUE)
Dat.URL <- "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"
DF <- read.csv(Dat.URL, header = FALSE, sep = ",", quote = "\"'")
names(DF) <- c('id_number', 'diagnosis', 'radius_mean',
'texture_mean', 'perimeter_mean', 'area_mean',
'smoothness_mean', 'compactness_mean',
'concavity_mean','concave_points_mean',
'symmetry_mean', 'fractal_dimension_mean',
'radius_se', 'texture_se', 'perimeter_se',
'area_se', 'smoothness_se', 'compactness_se',
'concavity_se', 'concave_points_se',
'symmetry_se', 'fractal_dimension_se',
'radius_worst', 'texture_worst',
'perimeter_worst', 'area_worst',
'smoothness_worst', 'compactness_worst',
'concavity_worst', 'concave_points_worst',
'symmetry_worst', 'fractal_dimension_worst')
DF$id_number <- NULL
head(DF)
write.csv(DF,"C:/Users/13363/Documents/Semesters/SPRING2024/Splunk/Code/Wisconsin_Bal1.csv", row.names = FALSE)
Dat.URL1 <- "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"
# myfile <- readLines(Dat.URL)
# head(myfile)
DF1 <- read.csv(Dat.URL1, header = FALSE, sep = ",", quote = "\"'")
names(DF1) <- c('id_number', 'diagnosis', 'radius_mean',
'texture_mean', 'perimeter_mean', 'area_mean',
'smoothness_mean', 'compactness_mean',
'concavity_mean','concave_points_mean',
'symmetry_mean', 'fractal_dimension_mean',
'radius_se', 'texture_se', 'perimeter_se',
'area_se', 'smoothness_se', 'compactness_se',
'concavity_se', 'concave_points_se',
'symmetry_se', 'fractal_dimension_se',
'radius_worst', 'texture_worst',
'perimeter_worst', 'area_worst',
'smoothness_worst', 'compactness_worst',
'concavity_worst', 'concave_points_worst',
'symmetry_worst', 'fractal_dimension_worst')
DF$id_number <- NULL
head(DF1)
DF1$diagnosis[DF1$diagnosis == "M"] = 1 # Cancer present
DF1$diagnosis[DF1$diagnosis == "B"] = 0 # Cancer Absent
DF1$diagnosis = as.factor(DF1$diagnosis)
write.csv(DF1,"C:/Users/13363/Documents/Semesters/SPRING2024/Splunk/Code/Wisconsin_Bal2.csv", row.names = FALSE)
DF.Gen <- readRDS("C:/Users/13363/Desktop/Proposal Result/Results/NewDissertCode/RealBCtcga/bcTCGA.rds")
DF.Gen[["X"]] <- scale(DF.Gen[["X"]], center = TRUE, scale = TRUE) # scale is generic function whose default method centers and/or scales the columns of a numeric matrix.If center is TRUE then centering is done by subtracting the column means (omitting NAs) of x from their corresponding columns, and if center is FALSE, no centering is done.
DF.Gen <- as.data.frame(cbind(DF.Gen[["y"]], DF.Gen[["X"]]))
colnames(DF.Gen)[1] <- "Y"
write.csv(DF.Gen,"C:/Users/13363/Documents/Semesters/SPRING2024/Splunk/Code/BCTCGA_Reg.csv", row.names = FALSE)
load("C:/Users/13363/Desktop/samples/SampleResults.RData")
View(iRF_scrn.mod)
View(iRF.mod)
View(LASSO_scrn.mod)
View(LASSO.mod)
View(LASSO.mod)
View(SCAD_scrn.mod)
View(SCAD.mod)
View(SCAD.mod)
View(MCP_scrn.mod)
View(MCP.mod)
View(LASSO.mod)
View(RAMP_LS.mod)
View(RAMP_LS.mod)
View(RAMP_LW.mod)
View(RAMP_LW_scrn.mod)
View(RAMP_LS_scrn.mod)
View(RAMP_MCPS_scrn.mod)
View(RF_scrn.mod)
View(RF.mod)
?pbinom
?SBI
library(energy)
?SBI
####library####
library(snowfall)
?SBI
####library####
library(snowfall)
library(MASS)
sfInit(parallel=TRUE, cpus=15)
sfLibrary(energy)
sfLibrary(SBISIS)
?SBISIS
install.packages(c("bayestestR", "BH", "bookdown", "brew", "brio", "callr", "cli", "coda", "commonmark", "curl", "data.table", "datawizard", "DBI", "dbplyr", "desc", "DiagrammeR", "dials", "digest", "dslabs", "e1071", "emmeans", "estimability", "excel.link", "fansi", "float", "future", "future.apply", "gbm", "gdtools", "GGally", "ggforce", "ggplot2", "ggraph", "ggrepel", "ggthemes", "globals", "glue", "gmp", "graphlayouts", "gt", "hardhat", "Hmisc", "hrbrthemes", "htmlwidgets", "httpuv", "igraph", "infer", "insight", "ISOcodes", "jagsUI", "kableExtra", "later", "lava", "lavaan", "lavaanPlot", "lisrelToR", "listenv", "magick", "maps", "markdown", "Matrix", "MatrixExtra", "matrixStats", "modeldata", "paletteer", "parallelly", "parameters", "parsnip", "patchwork", "performance", "pkgbuild", "pkgload", "plotly", "processx", "progress", "ps", "psych", "ragg", "Rcpp", "RcppArmadillo", "RcppEigen", "RCurl", "readr", "recipes", "remotes", "renv", "reprex", "reticulate", "rgl", "rlang", "rmarkdown", "Rmpfr", "roxygen2", "rpart.plot", "rsconnect", "RUnit", "rvest", "s2", "sandwich", "sass", "seriation", "sf", "shape", "SimDesign", "StanHeaders", "stringi", "svglite", "systemfonts", "tictoc", "tidygraph", "tidyr", "tidyselect", "timechange", "timeDate", "timeSeries", "tinytex", "tm", "topicmodels", "tweenr", "ucminf", "usethis", "uuid", "V8", "viridis", "vroom", "withr", "workflows", "xfun", "xgboost", "XML", "xts", "yaml", "yardstick", "zip"))
install.packages(c("cli", "digest", "fansi", "glue", "httpuv", "later", "Rcpp", "rlang", "stringi", "xfun", "yaml"))
library(mvtnorm)
library(energy)
?creat.sigma1
?bcor
?dcor
install.packages("Ball")
library(Ball)
?bcor
###library####
library(snowfall)
library(mvtnorm)
library(Ball)
library(energy) # for distance correlation
library(parallel)
library(doParallel)
################################################################################
############## Checking system ability (Number or Cores if any) ################
################################################################################
(core.num <- detectCores())
set.seed(1788)
################################################################################
################################ SIMULATED DATA ################################
################################################################################
simulate_data <- function(n, p, rho) {
Sigma <- matrix(rho, nrow = p, ncol = p)
diag(Sigma) <- 1
X <- mvtnorm::rmvnorm(n, rep(0, p), Sigma)
epsilon <- rnorm(n)
Y <- 3*X[,1] + 0.8*X[,10] + 0.6*X[,20] - 1.5*X[,1]*X[,10] - 2*X[,1]*X[,20] + epsilon
return(list(X = X, Y = Y))
}
SIS.func <- function(n, p, rho) {
data <- simulate_data(n, p, rho)
X <- data$X
Y <- data$Y
# Calculate correlation for each predictor
corrs <- abs(cor(X, Y))
# Rank predictors based on their absolute correlations
ranked_indices <- order(corrs, decreasing = TRUE)
# Considering interactions among the top-ranked predictors
top_predictors <- X[, ranked_indices[1:10]]
interaction_terms <- NULL
interactions_list <- list()
for (i in 1:ncol(top_predictors)) {
for (j in i:ncol(top_predictors)) {
interaction_terms <- cbind(interaction_terms, top_predictors[,i] * top_predictors[,j])
interactions_list[[length(interactions_list) + 1]] <- paste(ranked_indices[i], ranked_indices[j], sep="*")
}
}
interaction_corrs <- if (!is.null(interaction_terms)) abs(cor(interaction_terms, Y)) else numeric()
all_corrs <- c(corrs, interaction_corrs)
# Screened indices considering both predictors and interactions
screened_indices <- order(all_corrs, decreasing = TRUE)[1:p]
# Simulate a selection result structure compatible with the defined functions
selection_results <- list(screened_indices)
active_predictors <- c(1, 10, 20) # Active predictors based on the simulation function
true_interactions <- c("1*10", "1*20") # True interactions as strings to match the interactions_list structure
# Calculate metrics using the defined functions
M <- calculate_M(selection_results, active_predictors)
sel.rate <- length(intersect(ranked_indices[1:10], screened_indices)) / 10
# Direct calculation for sensitivity and specificity since the defined functions expect multiple models
identified_interactions <- lapply(screened_indices[screened_indices > p] - p, function(x) interactions_list[[x]])
# Adjust the specificity and sensitivity calculations to fit this context
specificity <- calculate_specificity(list(identified_interactions), true_interactions)
sensitivity <- length(Filter(function(x) x %in% true_interactions, unlist(identified_interactions))) / length(true_interactions)
return(list(
screened_data = X[, screened_indices, drop = FALSE],
M = M,
sel.rate = sel.rate,
sensitivity = sensitivity,
specificity = specificity
))
}
# Example usage
n <- 100
p <- 50
rho <- 0.5
results <- SIS.func(n, p, rho)
################################################################################
############################## METRICS FUNCTION ################################
################################################################################
calculate_M <- function(selection_results, active_predictors) {
M_values <- sapply(selection_results, function(model) {
all_active_included <- all(active_predictors %in% model)
if (all_active_included) {
return(length(model))
} else {
return(NA)
}
})
min_M <- min(M_values, na.rm = TRUE)
return(min_M)
}
calculate_Mqtl <- function(M_distribution) {
quantiles <- quantile(M_distribution, probs = c(0.05, 0.25, 0.5, 0.75, 0.95))
return(quantiles)
}
calculate_selection_rate <- function(selection_results, active_predictors) {
rates <- sapply(active_predictors, function(predictor) {
selection_count <- sum(sapply(selection_results, function(model) predictor %in% model))
rate <- selection_count / length(selection_results)
return(rate)
})
names(rates) <- active_predictors
return(rates)
}
calculate_sensitivity <- function(selection_results, true_interactions) {
coverage_count <- sum(sapply(selection_results, function(model) {
all(true_interactions %in% model)
}))
sensitivity_rate <- coverage_count / length(selection_results)
return(sensitivity_rate)
}
calculate_specificity <- function(selection_results, true_interactions) {
specificity_count <- sum(sapply(selection_results, function(model) {
correct_interactions <- all(true_interactions %in% model)
no_extra_interactions <- all(model[model %in% true_interactions] %in% true_interactions)
correct_and_no_extra <- correct_interactions && no_extra_interactions
return(correct_and_no_extra)
}))
specificity_rate <- specificity_count / length(selection_results)
return(specificity_rate)
}
SIS.func <- function(n, p, rho) {
data <- simulate_data(n, p, rho)
X <- data$X
Y <- data$Y
# Calculate correlation for each predictor
corrs <- abs(cor(X, Y))
# Rank predictors based on their absolute correlations
ranked_indices <- order(corrs, decreasing = TRUE)
# Considering interactions among the top-ranked predictors
top_predictors <- X[, ranked_indices[1:10]]
interaction_terms <- NULL
interactions_list <- list()
for (i in 1:ncol(top_predictors)) {
for (j in i:ncol(top_predictors)) {
interaction_terms <- cbind(interaction_terms, top_predictors[,i] * top_predictors[,j])
interactions_list[[length(interactions_list) + 1]] <- paste(ranked_indices[i], ranked_indices[j], sep="*")
}
}
interaction_corrs <- if (!is.null(interaction_terms)) abs(cor(interaction_terms, Y)) else numeric()
all_corrs <- c(corrs, interaction_corrs)
# Screened indices considering both predictors and interactions
screened_indices <- order(all_corrs, decreasing = TRUE)[1:p]
# Simulate a selection result structure compatible with the defined functions
selection_results <- list(screened_indices)
active_predictors <- c(1, 10, 20) # Active predictors based on the simulation function
true_interactions <- c("1*10", "1*20") # True interactions as strings to match the interactions_list structure
# Calculate metrics using the defined functions
M <- calculate_M(selection_results, active_predictors)
sel.rate <- length(intersect(ranked_indices[1:10], screened_indices)) / 10
# Direct calculation for sensitivity and specificity since the defined functions expect multiple models
identified_interactions <- lapply(screened_indices[screened_indices > p] - p, function(x) interactions_list[[x]])
# Adjust the specificity and sensitivity calculations to fit this context
specificity <- calculate_specificity(list(identified_interactions), true_interactions)
sensitivity <- length(Filter(function(x) x %in% true_interactions, unlist(identified_interactions))) / length(true_interactions)
return(list(
screened_data = X[, screened_indices, drop = FALSE],
M = M,
sel.rate = sel.rate,
sensitivity = sensitivity,
specificity = specificity
))
}
# Example usage
n <- 100
p <- 50
rho <- 0.5
results <- SIS.func(n, p, rho)
data <- simulate_data(n, p, rho)
X <- data$X
Y <- data$Y
# Calculate correlation for each predictor
corrs <- abs(cor(X, Y))
# Rank predictors based on their absolute correlations
ranked_indices <- order(corrs, decreasing = TRUE)
# Considering interactions among the top-ranked predictors
top_predictors <- X[, ranked_indices[1:10]]
interaction_terms <- NULL
interactions_list <- list()
for (i in 1:ncol(top_predictors)) {
for (j in i:ncol(top_predictors)) {
interaction_terms <- cbind(interaction_terms, top_predictors[,i] * top_predictors[,j])
interactions_list[[length(interactions_list) + 1]] <- paste(ranked_indices[i], ranked_indices[j], sep="*")
}
}
interaction_corrs <- if (!is.null(interaction_terms)) abs(cor(interaction_terms, Y)) else numeric()
all_corrs <- c(corrs, interaction_corrs)
# Screened indices considering both predictors and interactions
screened_indices <- order(all_corrs, decreasing = TRUE)[1:p]
# Simulate a selection result structure compatible with the defined functions
selection_results <- list(screened_indices)
active_predictors <- c(1, 10, 20) # Active predictors based on the simulation function
true_interactions <- c("1*10", "1*20") # True interactions as strings to match the interactions_list structure
# Calculate metrics using the defined functions
M <- calculate_M(selection_results, active_predictors)
sel.rate <- length(intersect(ranked_indices[1:10], screened_indices)) / 10
# Direct calculation for sensitivity and specificity since the defined functions expect multiple models
identified_interactions <- lapply(screened_indices[screened_indices > p] - p, function(x) interactions_list[[x]])
# Adjust the specificity and sensitivity calculations to fit this context
specificity <- calculate_specificity(list(identified_interactions), true_interactions)
sensitivity <- length(Filter(function(x) x %in% true_interactions, unlist(identified_interactions))) / length(true_interactions)
dim(X)
length(screened_indices)
library(glmnet)
ISIS.func <- function(n, p, rho) {
data <- simulate_data(n, p, rho)
X <- data$X
Y <- data$Y
# Initial SIS step
corrs <- abs(cor(X, Y))
ranked_indices <- order(corrs, decreasing = TRUE)[1:round(p/2)]
X_sis <- X[, ranked_indices]
# First round of Lasso
lasso_mod <- glmnet(as.matrix(X_sis), Y, alpha = 1)
cv.lasso <- cv.glmnet(as.matrix(X_sis), Y, alpha = 1)
lambda_min <- cv.lasso$lambda.min
coef_lasso <- predict(lasso_mod, type = "coefficients", s = lambda_min)[,1]
active_vars <- which(coef_lasso != 0) - 1 # Adjusting for intercept
# Generate and select interactions
X_interaction <- NULL
interaction_terms_index <- c()
for (i in 1:length(active_vars)) {
for (j in i:length(active_vars)) {
interaction_term <- X_sis[,i] * X_sis[,j]
X_interaction <- cbind(X_interaction, interaction_term)
interaction_terms_index <- c(interaction_terms_index, paste(active_vars[i], active_vars[j], sep="*"))
}
}
# Combining for final model
X_final <- cbind(X_sis, X_interaction)
# Second Lasso with interactions
final_lasso_mod <- glmnet(as.matrix(X_final), Y, alpha = 1)
final_cv.lasso <- cv.glmnet(as.matrix(X_final), Y, alpha = 1)
final_lambda_min <- final_cv.lasso$lambda.min
final_coef_lasso <- predict(final_lasso_mod, type = "coefficients", s = final_lambda_min)[,1]
final_active_vars <- which(final_coef_lasso != 0) - 1 # Adjusting for intercept
# Preparing selection results structure for calculate_M and other functions
selection_results <- list(final_active_vars)
active_predictors <- c(1, 10, 20) # Based on simulation function
true_interactions <- c("1*10", "1*20")
# Applying defined functions for metrics
M <- calculate_M(selection_results, active_predictors)
# Sensitivity and Specificity calculations
sensitivity <- calculate_sensitivity(list(interaction_terms_index[final_active_vars[length(active_vars)+1:length(final_active_vars)]]), true_interactions)
specificity <- calculate_specificity(list(interaction_terms_index[final_active_vars[length(active_vars)+1:length(final_active_vars)]]), true_interactions)
# Selection Rate is calculated directly, as sel.rate function needs adaptation for single-run context
sel.rate <- length(intersect(ranked_indices, final_active_vars)) / length(ranked_indices)
return(list(
screened_data = X_final[, final_active_vars, drop = FALSE],
M = M,
sel.rate = sel.rate,
sensitivity = sensitivity,
specificity = specificity
))
}
n <- 100
p <- 50
rho <- 0.5
results <- ISIS.func(n, p, rho)
################################################################################
############################## METRICS FUNCTION ################################
################################################################################
calculate_M <- function(selection_results, active_predictors) {
M_values <- sapply(selection_results, function(model) {
all_active_included <- all(active_predictors %in% model)
if (all_active_included) {
return(length(model))
} else {
return(NA)
}
})
min_M <- min(M_values)
return(min_M)
}
library(glmnet)
ISIS.func <- function(n, p, rho) {
data <- simulate_data(n, p, rho)
X <- data$X
Y <- data$Y
# Initial SIS step
corrs <- abs(cor(X, Y))
ranked_indices <- order(corrs, decreasing = TRUE)[1:round(p/2)]
X_sis <- X[, ranked_indices]
# First round of Lasso
lasso_mod <- glmnet(as.matrix(X_sis), Y, alpha = 1)
cv.lasso <- cv.glmnet(as.matrix(X_sis), Y, alpha = 1)
lambda_min <- cv.lasso$lambda.min
coef_lasso <- predict(lasso_mod, type = "coefficients", s = lambda_min)[,1]
active_vars <- which(coef_lasso != 0) - 1 # Adjusting for intercept
# Generate and select interactions
X_interaction <- NULL
interaction_terms_index <- c()
for (i in 1:length(active_vars)) {
for (j in i:length(active_vars)) {
interaction_term <- X_sis[,i] * X_sis[,j]
X_interaction <- cbind(X_interaction, interaction_term)
interaction_terms_index <- c(interaction_terms_index, paste(active_vars[i], active_vars[j], sep="*"))
}
}
# Combining for final model
X_final <- cbind(X_sis, X_interaction)
# Second Lasso with interactions
final_lasso_mod <- glmnet(as.matrix(X_final), Y, alpha = 1)
final_cv.lasso <- cv.glmnet(as.matrix(X_final), Y, alpha = 1)
final_lambda_min <- final_cv.lasso$lambda.min
final_coef_lasso <- predict(final_lasso_mod, type = "coefficients", s = final_lambda_min)[,1]
final_active_vars <- which(final_coef_lasso != 0) - 1 # Adjusting for intercept
# Preparing selection results structure for calculate_M and other functions
selection_results <- list(final_active_vars)
active_predictors <- c(1, 10, 20) # Based on simulation function
true_interactions <- c("1*10", "1*20")
# Applying defined functions for metrics
M <- calculate_M(selection_results, active_predictors)
# Sensitivity and Specificity calculations
sensitivity <- calculate_sensitivity(list(interaction_terms_index[final_active_vars[length(active_vars)+1:length(final_active_vars)]]), true_interactions)
specificity <- calculate_specificity(list(interaction_terms_index[final_active_vars[length(active_vars)+1:length(final_active_vars)]]), true_interactions)
# Selection Rate is calculated directly, as sel.rate function needs adaptation for single-run context
sel.rate <- length(intersect(ranked_indices, final_active_vars)) / length(ranked_indices)
return(list(
screened_data = X_final[, final_active_vars, drop = FALSE],
M = M,
sel.rate = sel.rate,
sensitivity = sensitivity,
specificity = specificity
))
}
n <- 100
p <- 50
rho <- 0.5
results <- ISIS.func(n, p, rho)
print(paste("Minimum Model Size (M):", results$M))
print("Quantiles of the Minimum Model Size (Mqtl):")
print(results$Mqtl)
print(paste("Selection Rate (sel.rate):", results$sel.rate))
print(paste("Sensitivity (Interaction Coverage Rate):", results$sensitivity))
print(paste("Specificity (Exact Interaction Terms Selection Rate):", results$specificity))
install.packages(c("arm", "bifurcatingr", "broom.helpers", "bslib", "callr", "cli", "crul", "data.table", "datawizard", "digest", "effectsize", "emmeans", "fansi", "future", "future.apply", "ggstats", "gh", "glue", "htmltools", "httpuv", "httr2", "infer", "insight", "knitr", "later", "lme4", "munsell", "parsnip", "performance", "pkgdown", "promises", "quanteda", "Rcpp", "RcppArmadillo", "rlang", "rsample", "rsconnect", "rstudioapi", "sf", "shiny", "stringi", "tidymodels", "tidytext", "tune", "workflowsets", "xfun", "yaml", "yardstick"))
install.packages(c("arm", "bifurcatingr", "broom.helpers", "bslib", "callr", "cli", "crul", "data.table", "datawizard", "digest", "effectsize", "emmeans", "fansi", "future", "future.apply", "ggstats", "gh", "glue", "htmltools", "httpuv", "httr2", "infer", "insight", "knitr", "later", "lme4", "munsell", "parsnip", "performance", "pkgdown", "promises", "quanteda", "Rcpp", "RcppArmadillo", "rlang", "rsample", "rsconnect", "rstudioapi", "sf", "shiny", "stringi", "tidymodels", "tidytext", "tune", "workflowsets", "xfun", "yaml", "yardstick"))
setwd("C:/Users/13363/Documents/Semesters/SPRING2024/Paper1_Doc/Paper1/Submitted-Material/Code")
